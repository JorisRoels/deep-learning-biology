{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bSg5U4D5I35S"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JorisRoels/deep-learning-biology/blob/main/exercises/assignments/2020-dlb-0-intro-pytorch.ipynb)\n",
    "\n",
    "# Exercise 0: Introduction to PyTorch\n",
    "\n",
    "All of the practical sessions will be largely based on [PyTorch](https://pytorch.org/), a high-level deep learning library. PyTorch is a well-known, open-source, machine learning framework that has a comprehensive set of tools and libraries and accelerates research prototyping. It also supports transparant training of machine learning models on GPU devices, which can benefit runtimes significantly. The full documentation can be found [here](https://pytorch.org/docs/stable/index.html). In this notebook, we will be getting familiar with PyTorch, a popular deep learning library. \n",
    "\n",
    "The structure of these exercises is as follows: \n",
    "\n",
    "0. [Installation (optional)](#scrollTo=ScagUEMTMjlK)\n",
    "1. [Import libraries](#scrollTo=ScagUEMTMjlK)\n",
    "2. [Data in Pytorch: Tensors](#scrollTo=ohZHyOTnI35b)\n",
    "3. [Datasets and dataloaders](#scrollTo=kIry8iFZI35y)\n",
    "4. [Computational models](#scrollTo=uXrEb0rTI35-)\n",
    "\n",
    "This notebook is largely based on the [PyTorch tutorials](https://pytorch.org/tutorials/beginner/basics/intro.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Installation (optional)\n",
    "For these exercise sessions, you can either choose to work locally on your own laptop, or remote in Google Colab. This section is required for those who would like to work locally. \n",
    "\n",
    "To get started with the exercise sessions, your environment should satisfy the following prerequisites: \n",
    "1. Check whether you have a [CUDA enabled GPU]. If not, you will have to work on the CPU, which will be significantly slower. we highly suggest to use Google Colab for these sessions in that case. \n",
    "2. Python 3.6 or higher ([install](https://www.python.org/)) - version check: `python --version`\n",
    "3. A package manager: Anaconda [install](https://www.anaconda.com/) or pip [install](https://pypi.org/project/pip/) - pip comes along with the suggested installer from the previous step. In that case you can skip this step. \n",
    "3. CUDA ([install](https://developer.nvidia.com/cuda-zone)) - version check: `nvcc --version`\n",
    "4. PyTorch ([install](https://pytorch.org/)) - select your OS, package manager and compute platform (CUDA version) and run the suggested command. \n",
    "5. To ensure a correct installation, you should be able to run the following cell without errors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f'Succesfully loaded the PyTorch module! ')\n",
    "print(f'CUDA available? {torch.cuda.is_available()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are now ready to do the exercises locally! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ScagUEMTMjlK"
   },
   "source": [
    "## 1. Import libraries\n",
    "Let's start with importing the necessary libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1152,
     "status": "ok",
     "timestamp": 1607183794439,
     "user": {
      "displayName": "Joris Roels",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiO17HAi3vdM28rwcREixXOvi5CItNho4sxx-YCwkU=s64",
      "userId": "00181780506485155029"
     },
     "user_tz": -60
    },
    "id": "S1oGi88ZU8eN"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcdefaults()\n",
    "%matplotlib notebook\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yIPng9wbV-Zs"
   },
   "source": [
    "As you will notice, Colab environments come with quite a large library pre-installed. If you need to import a module that is not yet specified, you can add it in the previous cell (make sure to run it again). If the module is not installed, you can install it with `pip`. \n",
    "\n",
    "To make your work reproducible, it is advised to initialize all modules with stochastic functionality with a fixed seed. Re-running this script should give the same results as long as the seed is fixed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1463,
     "status": "ok",
     "timestamp": 1607183794755,
     "user": {
      "displayName": "Joris Roels",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiO17HAi3vdM28rwcREixXOvi5CItNho4sxx-YCwkU=s64",
      "userId": "00181780506485155029"
     },
     "user_tz": -60
    },
    "id": "8OOFPFLiV-mh",
    "outputId": "664cfd92-aed4-46c4-9cde-0d5a8c699082"
   },
   "outputs": [],
   "source": [
    "# make sure the results are reproducible\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PjfnM2ffU9G0"
   },
   "source": [
    "To be able to perform calculations in parallel, you will need a decent GPU device. Let's find out which devices are available in this environment. PyTorch can help you with this. \n",
    "\n",
    "**Exercise**: \n",
    "- Find out which devices are available using the functions available in the [`torch.cuda`](https://pytorch.org/docs/stable/cuda.html) module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the amount of available devices\n",
    "n_devices = torch.cuda.device_count()\n",
    "print(f'Number of available devices: {n_devices}')\n",
    "\n",
    "# get stats of each available device\n",
    "for i in range(n_devices):\n",
    "    device = torch.cuda.device(i)\n",
    "    print(torch.cuda.get_device_properties(device))\n",
    "\n",
    "# select the first GPU as our target device\n",
    "device = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on your hardware configuration, you will see the available devices. In Google Colab, you should have at least one GPU available. If not, go to \"Runtime > Change runtime type\" and make sure that GPU is selected as hardware accelerator (adjustments will require a session restart). \n",
    "\n",
    "You are now ready to do some calculations using the selected GPU device in PyTorch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kIry8iFZI35y"
   },
   "source": [
    "## 2. Data in PyTorch: Tensors\n",
    "\n",
    "When you think about representing data in Python, you will probably think about numpy arrays (i.e. `ndarray` objects) and the many useful operations that can be performed with these data structures. Unfortunately, numpy does not allow computations on accelerated devices such as GPUs. PyTorch aims to resolve this issue, while also being fully compatible with numpy. Instead of `ndarray` objects, PyTorch uses `Tensor` objects, which are basically multidimensional arrays.  \n",
    "\n",
    "Let's have a look at an example. In the code snippet below, we implement a dot product between two vectors in numpy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22474,
     "status": "ok",
     "timestamp": 1607183815789,
     "user": {
      "displayName": "Joris Roels",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiO17HAi3vdM28rwcREixXOvi5CItNho4sxx-YCwkU=s64",
      "userId": "00181780506485155029"
     },
     "user_tz": -60
    },
    "id": "dhB6nbkXI35z",
    "outputId": "8933194b-fe5e-40cf-f9dd-4dc4491b67ed"
   },
   "outputs": [],
   "source": [
    "# initialize the two vectors\n",
    "a = np.array([1., 2., 3., 4.])\n",
    "b = np.array([5., 6., 7., 8.])\n",
    "print(a)\n",
    "print(b)\n",
    "\n",
    "# compute the dot product\n",
    "dot = np.dot(a, b)\n",
    "print(dot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WRgIWPOwI354"
   },
   "source": [
    "In PyTorch we can do this using a very similar syntax. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the two vectors\n",
    "a = torch.tensor([1., 2., 3., 4.])\n",
    "b = torch.tensor([5., 6., 7., 8.])\n",
    "print(a)\n",
    "print(b)\n",
    "\n",
    "# compute the dot product\n",
    "dot = torch.dot(a, b)\n",
    "print(dot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To port the numpy code to PyTorch, you basically have to change `np.array` into `torch.tensor` and `np` into `torch`. Note this will definitely not always be the case as there are still some differences in the interface. \n",
    "\n",
    "However, now we can run the PyTorch code on the GPU by simply calling the `to` method of the Tensor class with the device index that we selected just before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the two vectors\n",
    "a = torch.tensor([1., 2., 3., 4.]).to(device)\n",
    "b = torch.tensor([5., 6., 7., 8.]).to(device)\n",
    "print(a)\n",
    "print(b)\n",
    "\n",
    "# compute the dot product\n",
    "dot = torch.dot(a, b)\n",
    "print(dot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very straightforward to perform calculations on the GPU! Note: if you want to run the computations in PyTorch on CPU, you can always set `device = 'cpu'`. Switching between tensors and numpy arrays is simple: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert a tensor to a numpy array\n",
    "a = a.to('cpu').numpy()\n",
    "print(a.__class__)\n",
    "\n",
    "# convert a numpy array to a tensor\n",
    "a = torch.from_numpy(a)\n",
    "print(a.__class__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind that you first have to transfer the tensor to the CPU before you convert it into a numpy array. \n",
    "\n",
    "You can initialize tensors in various ways: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# desired shape of the tensor\n",
    "shape = (2, 3)\n",
    "\n",
    "# initialization\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, tensors have various attributes that you can use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.rand(3,4)\n",
    "\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's your time to experiment with PyTorch! \n",
    "\n",
    "**Exercise**: get familiar with the PyTorch documentation. \n",
    "- Implement the (numpy based) code below using PyTorch. \n",
    "- Transfer all tensors to a GPU device. \n",
    "- Wrap both the numpy and pytorch code within a for loop and measure the runtime over a number of repetitions (you can use the `time.time()` function). Experiment with the amount of repetitions and the size of the arrays/tensors. How do numpy and PyTorch compare? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform various operations on these matrices\n",
    "def numpy_ops(X, Y):\n",
    "    W = np.matmul(X, Y)\n",
    "    W = W - np.mean(X)\n",
    "    W = W / np.std(Y)\n",
    "    W = np.sqrt(np.abs(W))\n",
    "    return W\n",
    "\n",
    "# size of the matrix\n",
    "n = 4\n",
    "\n",
    "# randomly define two matrices\n",
    "X = np.random.randn(n, n)\n",
    "Y = np.random.randn(n, n)\n",
    "W = numpy_ops(X, Y)\n",
    "print(X)\n",
    "print(Y)\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform various operations on these matrices\n",
    "def torch_ops(X, Y):\n",
    "    '''\n",
    "    INSERT CODE HERE\n",
    "    '''\n",
    "\n",
    "'''\n",
    "INSERT CODE HERE\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22470,
     "status": "ok",
     "timestamp": 1607183815790,
     "user": {
      "displayName": "Joris Roels",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiO17HAi3vdM28rwcREixXOvi5CItNho4sxx-YCwkU=s64",
      "userId": "00181780506485155029"
     },
     "user_tz": -60
    },
    "id": "n8l0jAmOI356",
    "outputId": "cd3acbbe-89e9-4fc0-9f86-e08337da8786"
   },
   "source": [
    "We can draw two main conclusions from these experiments: \n",
    "- The PyTorch implementation scales significantly better towards larger inputs due to the large amounts of operations that can be performed in parallel. GPUs are ideal for parallel computing, whereas classical CPUs are more suitable for sequential processing. \n",
    "- For smaller inputs, it can be that the numpy implementation outperforms PyTorch. This is largely due to the fact that memory transfers between the CPU and GPU cause too much overhead compared to the parallelism performance gain. Before deploying models on a GPU device, it is therefore crucial to have sufficiently large datasets. \n",
    "\n",
    "Now that we know how tensors are used in PyTorch, we are ready to work with actual data in so-called datasets and dataloaders. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uXrEb0rTI35-"
   },
   "source": [
    "## 4. Datasets and dataloaders\n",
    "\n",
    "Ideally, you would like to decouple data from your predictive model. This would allow you to easily try out a specific model on a different dataset (or another model on the same dataset). The data management part that enables this in PyTorch is handled by the `torch.utils.data.Dataset` and `torch.utils.data.DataLoader` classes. `Dataset` stores the samples (and, optionally, their corresponding labels), and `DataLoader` wraps an iterable around the `Dataset` to enable easy access to the samples.\n",
    "\n",
    "Lots of commonly used datasets are available in PyTorch. Check out the [documentation](https://pytorch.org/vision/stable/datasets.html) for a full overview. For example, let's try and load the well-known MNIST dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you haven't downloaded this dataset yet in the specified root directory, this will download and extract it. Note that this dataset consists of a training and testing part. \n",
    "\n",
    "The `training_data` and `test_data` objects inherit from the `torch.utils.data.Dataset` and can be used for sampling through indexing, similar to lists. In other words, `training_data[i]` will return the `i`th training sample of the MNIST dataset, which is a tuple of an image of a digit and the corresponding label. The following code illustrates this principle and shows a few examples of the MNIST dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
    "    img, label = training_data[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(label)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a closer look at these datasets by printing out all their variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the data and corresponding target labels are stored in memory as tensors. \n",
    "\n",
    "It is more likely that you would like to try out PyTorch models on your own data, instead of public datasets such as MNIST. To do this, you simply have to define a new class that inherits from `torch.utils.data.Dataset` and implement three functions: \n",
    "- `__init__`: this is the initializer of the class and will be called whenever a new instance of the class is generated. Common tasks that are performed in this method include loading the data (if they fit in memory), data pre-processing, etc. \n",
    "- `__len__`: this function returns the length of the dataset. \n",
    "- `__getitem__`: this function takes an index `i` as argument and returns the `i`th sample. \n",
    "\n",
    "**Exercise**: implement your own dataset that contains randomly generated data and labels by filling in the following skeleton code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, n_samples=100, n_classes=10, data_dim=20):\n",
    "        '''\n",
    "        INSERT CODE HERE\n",
    "        '''\n",
    "    \n",
    "    def __len__(self):\n",
    "        '''\n",
    "        INSERT CODE HERE\n",
    "        '''\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        '''\n",
    "        INSERT CODE HERE\n",
    "        '''\n",
    "\n",
    "data = CustomDataset(n_samples=100, n_classes=10, data_dim=20)\n",
    "x, y = data[0]\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent! Now we are ready to define computational models in PyTorch! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Computational models\n",
    "\n",
    "PyTorch uses the so-called `torch.nn.Module` class to represent functions that take one or more inputs and generate one or more outputs. Neural networks, as we will see in the following sessions, also fall within this category. \n",
    "\n",
    "As an example, let's look at a simple function that takes a vector as input, multiplies it with a number and adds a number to the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.m = nn.Parameter(torch.randn(1))\n",
    "        self.b = nn.Parameter(torch.randn(1))\n",
    "    \n",
    "    def forward(self, x): \n",
    "        return self.m*x + self.b\n",
    "\n",
    "f = LinearModel()\n",
    "x = torch.randn(1)\n",
    "y = f(x)\n",
    "\n",
    "print(f'm={float(f.m)}')\n",
    "print(f'b={float(f.b)}')\n",
    "print(f'x={float(x)}')\n",
    "print(f'y={float(y)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever you implement a `Module`, you have to implement the `forward` function as a minimum. This function defines the so-called forward propagation, i.e. the calculations necessary to generate an output for a given input. By using a `Module` as a function on a specific input `x` (i.e. `f(x)`), its `forward function` will be called with that particular input as an argument. Optionally, you can also define an initializer, which will be called whenever you instantiate the Module. Typically, this is used to initialize all necessary parameters of the module. In fact, we can explicitely tell the module that these numbers are parameters by wrapping them in an `torch.nn.Parameter` object. This enables optimization of these values later on. \n",
    "\n",
    "You might wonder why we can't simply define the linear model as a function in Python, which would require much less lines of code. The answer is in the automatic differentiation that is implemented in PyTorch. Let's explain this through an example. \n",
    "\n",
    "Consider a least squares linear regression problem, i.e. we have samples $D = \\{(x_i, y_i)\\}_{i=1, \\dots, n}$ that approximately follow a straight line. In other words: \n",
    "$$\n",
    "y_i = m x_i + b + \\varepsilon_i\n",
    "$$\n",
    "where $\\varepsilon_i$ is a small noise perturbation. The following code generates such a dataset $D$ and visualizes it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data parameters\n",
    "n = 100\n",
    "sigma_noise = 0.5\n",
    "\n",
    "# actual line parameters (y = m*x + b)\n",
    "m_real = 3.1\n",
    "b_real = -2.2\n",
    "\n",
    "# random numbers uniformly distributed in [-1, 1]\n",
    "x = 2*torch.rand(n) - 1\n",
    "# noisy data along a straight line\n",
    "y = m_real*x + b_real + sigma_noise*torch.randn(x.shape)\n",
    "\n",
    "# show the data\n",
    "def show_data(x, y, f=None):\n",
    "    x_ = torch.tensor([-1, 1])\n",
    "    plt.scatter(x, y, c='tab:blue')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    if f is not None:\n",
    "        y_ = f(x_)\n",
    "        plt.plot(x_, m_real*x_ + b_real, 'tab:red')\n",
    "        plt.plot(x_, y_.detach(), 'tab:green')\n",
    "        plt.legend(('Ground truth line', 'Estimated line', 'Samples'))\n",
    "    else:\n",
    "        plt.plot(x_, m_real*x_ + b_real, 'tab:red')\n",
    "        plt.legend(('Ground truth line', 'Samples'))\n",
    "\n",
    "fig = plt.figure()\n",
    "show_data(x, y, f=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our current linear model is definitely not a good fit for the data. A least squares estimator aims to optimize this linear model by minimizing the mean squared error (MSE) between the predictions $f(x_i)$ of our model $f$ and the actual output values $y_i$, i.e.\n",
    "$$\n",
    "\\hat{f} = \\arg\\min_f \\frac{1}{n} \\sum_i (f(x_i) - y_i)^2\n",
    "$$\n",
    "\n",
    "This is relatively straightforward to implement in PyTorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a new model\n",
    "f = LinearModel()\n",
    "\n",
    "# optimization parameters\n",
    "lr = 0.1\n",
    "epochs = 75\n",
    "mse_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(f.parameters(), lr=lr)\n",
    "\n",
    "# start optimization\n",
    "epochs_rng = torch.arange(epochs)\n",
    "mses = torch.zeros(epochs)\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # zero out the gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # current predictions\n",
    "    y_pred = f(x)\n",
    "    \n",
    "    # compute error\n",
    "    mse = mse_fn(y_pred, y)\n",
    "    mses[epoch] = mse\n",
    "    \n",
    "    # backward propagation\n",
    "    mse.backward()\n",
    "    \n",
    "    # adjust parameters (f.m and f.b)\n",
    "    optimizer.step()\n",
    "\n",
    "    # show error and regression throughout training\n",
    "    plt.clf()\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_rng[:epoch+1], mses[:epoch+1].detach(), 'tab:blue')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    show_data(x, y, f=f)\n",
    "    plt.pause(0.01)\n",
    "    fig.canvas.draw()\n",
    "\n",
    "# print out results\n",
    "print(f'Estimated model parameters: y={float(f.m)}*x + {float(f.b)}')\n",
    "print(f'Actual linear model: y={m_real}*x + {b_real}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That seems a reasonable regression! Now what is exactly going on here? \n",
    "\n",
    "The idea is that we try to minimize an error function (i.e. the MSE) on the data. The minimization algorithm is called gradient descent (we'll elaborate on this in the next theory session) and often used in optimization problems. \n",
    "\n",
    "The issue is that gradient descent requires computing the derivative of the loss function with respect to all parameters in the model, which can be mathematically tricky. Fortunately, PyTorch can do this automatically as long as we define the computations of our model within a `Module`. This automatic differentiation is often refered to as their [autograd](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html) system. \n",
    "\n",
    "**Exercise**: implement a new `Module` that takes a vector as input, applies matrix multiplication, adds a vector to the result and finally applies a sigmoid function to each element. In other words: \n",
    "$$\n",
    "f(x) = \\sigma(Ax+b)\n",
    "$$\n",
    "where $\\sigma(z) = \\frac{1}{1+e^{-z}}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, n, m):\n",
    "        '''\n",
    "        INSERT CODE HERE\n",
    "        '''\n",
    "    \n",
    "    def forward(self, x): \n",
    "        '''\n",
    "        INSERT CODE HERE\n",
    "        '''\n",
    "\n",
    "n = 10\n",
    "m = 5\n",
    "f = SimpleNet(n, m)\n",
    "x = torch.randn(n)\n",
    "y = f(x)\n",
    "\n",
    "print(f'x={x}')\n",
    "print(f'y={y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You have just implemented the forward propagation step of a neural network! Thanks to the autograd system in PyTorch, we don't have to worry about the backward propagation. In the next theory session, we will see what backpropagation exactly means and how we can train a neural network. "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "2020-dlb-1-neural-networks-solution.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
