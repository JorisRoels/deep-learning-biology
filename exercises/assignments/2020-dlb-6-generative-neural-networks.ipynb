{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bSg5U4D5I35S"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JorisRoels/deep-learning-biology/blob/main/exercises/assignments/2020-dlb-6-generative-neural-networks.ipynb)\n",
    "\n",
    "# Exercise 6: Generative neural networks\n",
    "\n",
    "In this notebook, we will train a generative adversarial network to generate images of digits. Next, we will adapt this model to generate electron microscopy data.  \n",
    "\n",
    "The structure of these exercises is as follows: \n",
    "\n",
    "1. [Import libraries and download data](#scrollTo=ScagUEMTMjlK)\n",
    "2. [Designing a generative adversarial network](#scrollTo=KdbhNqzDZ2wm)\n",
    "3. [Training a generative adversarial network](#scrollTo=ffGI9IoAyYzt)\n",
    "4. [Adaptation to electron microscopy data](#scrollTo=ohZHyOTnI35b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ScagUEMTMjlK"
   },
   "source": [
    "## 1. Import libraries and download data\n",
    "Let's start with importing the necessary libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dS4jDyCl09w8"
   },
   "outputs": [],
   "source": [
    "!pip install neuralnets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S1oGi88ZU8eN"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcdefaults()\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from progressbar import ProgressBar, Percentage, Bar, ETA, FileTransferSpeed\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import gdown\n",
    "import zipfile\n",
    "import os\n",
    "import progressbar\n",
    "import time\n",
    "\n",
    "from neuralnets.util.io import read_tif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yIPng9wbV-Zs"
   },
   "source": [
    "As you will notice, Colab environments come with quite a large library pre-installed. If you need to import a module that is not yet specified, you can add it in the previous cell (make sure to run it again). If the module is not installed, you can install it with `pip`. \n",
    "\n",
    "To make your work reproducible, it is advised to initialize all modules with stochastic functionality with a fixed seed. Re-running this script should give the same results as long as the seed is fixed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8OOFPFLiV-mh"
   },
   "outputs": [],
   "source": [
    "# make sure the results are reproducible\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# run all computations on the GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Running computations with %s' % torch.device(device))\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_properties(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PjfnM2ffU9G0"
   },
   "source": [
    "We will now download the required data from a public Google Drive repository. The data is stored as a zip archive and automatically extracted to the `data` directory in the current directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZCysdV0oWvAt"
   },
   "outputs": [],
   "source": [
    "# fields\n",
    "url = 'http://data.bits.vib.be/pub/trainingen/DeepLearning/data-3.zip'\n",
    "cmp_data_path = 'data.zip'\n",
    "\n",
    "# download the compressed data\n",
    "gdown.download(url, cmp_data_path, quiet=False)\n",
    "\n",
    "# extract the data\n",
    "zip = zipfile.ZipFile(cmp_data_path)\n",
    "zip.extractall('')\n",
    "\n",
    "# remove the compressed data\n",
    "os.remove(cmp_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KdbhNqzDZ2wm"
   },
   "source": [
    "## 2. Designing a generative adversarial network\n",
    "\n",
    "To get started with generative adversarial networks, we will design a model for the MNIST dataset. The following code will download the data (if necessary) and initialize a data loader. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "76DjwCtuaweJ"
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "batch_size = 32\n",
    "\n",
    "# transform object that pre-processes the data\n",
    "transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=0.5, std=0.5)\n",
    "])\n",
    "\n",
    "# data loader object\n",
    "loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train=True, download=True, transform=transform), \n",
    "    batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# show a few samples\n",
    "n = 5\n",
    "for i in range(n): \n",
    "    plt.subplot(1, n, i+1)\n",
    "    x = loader.dataset.data[i]\n",
    "    plt.imshow(x.numpy(), cmap='gray')\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9xPrIBylcNIY"
   },
   "source": [
    "Note that we only load the training data, as testing is not really necessary for generative models. The objective of a generative model is to generate good samples, not to maximize clustering or classification performance. To define an objective function for a generative model, remains an open problem up to now. \n",
    "\n",
    "Also denote that we apply a transformation that converts the data to tensors and normalizes the data to the $[-1,1]$ range. We will exploit this in our network architecture to simplify data generation. \n",
    "\n",
    "We can now start building the generative adversarial network. As seen in the theory, the architecture consists of a generator and discriminator. Both networks consist of 4 linear layers with [leaky ReLU](https://pytorch.org/docs/stable/generated/torch.nn.LeakyReLU.html) activation (except for the output activations). The generator transforms a latent noise vector into an vector represented image. The discriminator takes an image as input and computes the likelihood that this sample is real. Note that the output activation of the generator is a [tanh](https://pytorch.org/docs/stable/generated/torch.tanh.html) function (conform with the inputs that have been scaled to $[-1,1]$), and the output activation of the discriminator is a [sigmoid](https://pytorch.org/docs/stable/generated/torch.sigmoid.html) function to correspond with a probability. The image below shows the complete GAN architecture (top: generator, bottom: discriminator) in full detail. \n",
    "\n",
    "![GAN scheme](https://i.ibb.co/v15VJph/scheme2.png)\n",
    "\n",
    "**Exercise**: Implement the above GAN architecture: \n",
    "- Implement the initialization and forward functions of the Generator and Discriminator classes. \n",
    "- Print out the architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TOvBdUhCdZL7"
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        INSERT CODE HERE\n",
    "        \"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        \"\"\"\n",
    "        INSERT CODE HERE\n",
    "        \"\"\"\n",
    "\n",
    "        return x\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        \"\"\"\n",
    "        INSERT CODE HERE\n",
    "        \"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        \"\"\"\n",
    "        INSERT CODE HERE\n",
    "        \"\"\"\n",
    "\n",
    "        return x\n",
    "        \n",
    "\"\"\"\n",
    "INSERT CODE HERE\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ffGI9IoAyYzt"
   },
   "source": [
    "## 3. Training a generative adversarial network\n",
    "\n",
    "Now comes the tricky part: training a GAN from scratch. \n",
    "\n",
    "**Exercise**: Train the GAN network on the MNIST dataset. The code below has most components filled to train a GAN network. Take your time and go through it. In each epoch, the discriminator is first trained w.r.t. its loss function. Next, the generator is updated w.r.t. its loss function. At the end of each epoch, the generator computes its generated images from a fixed noise vector and you will see the generated results. Keep in mind that an epoch might require a few seconds to finish due to the many forward and backward passes. \n",
    "- Complete the `train_discriminator` function by implementing what is mentioned in the comments. \n",
    "- Complete the `train_generator` function by implementing what is mentioned in the comments. \n",
    "- Analyse how the generated samples evolve during training\n",
    "- Study the training curves and explain why the discriminator/generator loss increases/decreases. Note that we plot half of the discriminator loss because this is originally the sum of the losses for a real and fake sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "enumZMpsz-KY"
   },
   "outputs": [],
   "source": [
    "# show the generated samples that correspond to a (5x5) batch of noise vectors\n",
    "def show_generated_samples(G, z):\n",
    "\n",
    "    # set generator to evaluation mode and run it\n",
    "    G.eval()\n",
    "    test_images = G(z)\n",
    "\n",
    "    # figure stuff\n",
    "    size_figure_grid = 5\n",
    "    fig, ax = plt.subplots(size_figure_grid, size_figure_grid, figsize=(5, 5))\n",
    "    for i in range(size_figure_grid):\n",
    "        for j in range(size_figure_grid):\n",
    "            ax[i, j].get_xaxis().set_visible(False)\n",
    "            ax[i, j].get_yaxis().set_visible(False)\n",
    "\n",
    "    # visualize the generated images\n",
    "    for k in range(5*5):\n",
    "        i = k // 5\n",
    "        j = k % 5\n",
    "        ax[i, j].cla()\n",
    "        ax[i, j].imshow(test_images[k, :].cpu().detach().view(28, 28).numpy(), cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "# train the discriminator for a single iteration\n",
    "def train_discriminator(D, G, x, D_optimizer): \n",
    "\n",
    "    # zero the gradients\n",
    "    D.zero_grad()\n",
    "\n",
    "    # domain labels: real corresponds to 1, fake corresponds to 0\n",
    "    y_real = torch.ones(x.size(0)).to(device)\n",
    "    y_fake = torch.zeros(x.size(0)).to(device)\n",
    "\n",
    "    # run the discriminator on the real sample and compute the loss\n",
    "    \"\"\"\n",
    "    INSERT CODE HERE\n",
    "    \"\"\"\n",
    "\n",
    "    # generate a fake sample\n",
    "    \"\"\"\n",
    "    INSERT CODE HERE\n",
    "    \"\"\"\n",
    "\n",
    "    # run the discriminator on the fake sample and compute the loss\n",
    "    \"\"\"\n",
    "    INSERT CODE HERE\n",
    "    \"\"\"\n",
    "\n",
    "    # sum the real and fake loss\n",
    "    D_train_loss = D_real_loss + D_fake_loss\n",
    "\n",
    "    # backpropagation on the discriminator and parameter update\n",
    "    D_train_loss.backward()\n",
    "    D_optimizer.step()\n",
    "\n",
    "    discriminator_loss = D_train_loss.item()\n",
    "\n",
    "    return discriminator_loss\n",
    "\n",
    "# train the generator for a single iteration\n",
    "def train_generator(D, G, G_optimizer): \n",
    "\n",
    "    # zero the gradients\n",
    "    G.zero_grad()\n",
    "    \n",
    "    # domain labels: real corresponds to 1, fake corresponds to 0\n",
    "    y = torch.ones(batch_size).to(device)\n",
    "\n",
    "\n",
    "    # generate a fake sample\n",
    "    \"\"\"\n",
    "    INSERT CODE HERE\n",
    "    \"\"\"\n",
    "\n",
    "    # run the discriminator on the fake sample and compute the loss\n",
    "    \"\"\"\n",
    "    INSERT CODE HERE\n",
    "    \"\"\"\n",
    "\n",
    "    # backpropagation on the generator and parameter update\n",
    "    G_fake_loss.backward()\n",
    "    G_optimizer.step()\n",
    "\n",
    "    generator_loss = G_fake_loss.item()\n",
    "\n",
    "    return generator_loss\n",
    "\n",
    "# train the GAN network for a number of epochs\n",
    "def train_gan(D, G, loader, D_optimizer, G_optimizer, epochs, fixed_z):\n",
    "\n",
    "    loss_d = np.zeros((epochs))\n",
    "    loss_g = np.zeros((epochs))\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # set generator and discriminator to training mode\n",
    "        G.train()\n",
    "        D.train()\n",
    "\n",
    "        for x, _ in loader:\n",
    "            \n",
    "            # get a batch of samples\n",
    "            x = x.view(-1, 28 * 28).to(device).float()\n",
    "\n",
    "            # train discriminator D and cumulate loss\n",
    "            loss = train_discriminator(D, G, x, D_optimizer)\n",
    "            loss_d[epoch] += loss\n",
    "\n",
    "            # train generator G and cumulate loss\n",
    "            loss = train_generator(D, G, G_optimizer)\n",
    "            loss_g[epoch] += loss\n",
    "\n",
    "        # average losses and report\n",
    "        loss_d[epoch] /= len(loader)\n",
    "        loss_g[epoch] /= len(loader)\n",
    "        print('Epoch %d: Discriminator loss = %.3f - Generator loss = %.3f' % (epoch, loss_d[epoch], loss_g[epoch]))\n",
    "\n",
    "        # show generated samples\n",
    "        show_generated_samples(G, fixed_z)\n",
    "\n",
    "    return loss_d, loss_g\n",
    "\n",
    "\n",
    "# training parameters\n",
    "lr = 2e-4\n",
    "epochs = 10\n",
    "\n",
    "# binary cross entropy loss is used in a GAN\n",
    "BCE_loss = nn.BCELoss()\n",
    "\n",
    "# adam optimizer for both networks\n",
    "G_optimizer = optim.Adam(G.parameters(), lr=lr)\n",
    "D_optimizer = optim.Adam(D.parameters(), lr=lr)\n",
    "\n",
    "# fixed noise (for consistency in comparing generated samples)\n",
    "fixed_z = torch.randn((5 * 5, G.input_size)).to(device)\n",
    "\n",
    "# train the network\n",
    "loss_d, loss_g = train_gan(D, G, loader, D_optimizer, G_optimizer, epochs, fixed_z)\n",
    "\n",
    "# visualize the learning curves\n",
    "plt.plot(loss_d / 2)\n",
    "plt.plot(loss_g)\n",
    "plt.legend(('Discriminator', 'Generator'))\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kBDZsGY9D96X"
   },
   "source": [
    "Keep in mind that training GAN networks can be tedious and time-consuming. Hyperparameter settings and architectural choices such as where to include dropout and normalization are crucial. Feel free to try this out by e.g. removing the dropout layers, changing the learning rate, etc.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ohZHyOTnI35b"
   },
   "source": [
    "## 4. Adaptation to electron microscopy data\n",
    "\n",
    "We will now adapt the GAN for digit image generation to a generative model for electron microscopy data. Specifically, the data used originates from the ISBI 2012 segmentation challenge on neuron structures (see third exercise session). As our model is unsupervised, we will not used the labels provided in the dataset. We briefly visualize a slice of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4hJjZQEGI35f"
   },
   "outputs": [],
   "source": [
    "# specify where the data is stored\n",
    "data_dir = 'data-3'\n",
    "\n",
    "# load the datadispensers\n",
    "x = read_tif(os.path.join(data_dir, 'train-volume.tif'))\n",
    "\n",
    "# print out size\n",
    "print('Size of the volume: %d x %d x %d' % x.shape)\n",
    "\n",
    "# show example\n",
    "plt.imshow(x[0], cmap='gray')\n",
    "plt.title('Input data')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hv8mj67MS8gR"
   },
   "source": [
    "We will try to re-use the code from the MNIST dataset as much as possible. To make our lives easier, we will therefore attempt to generate EM samples of size $28 \\times 28$ as well. To do this, we need a data loader that samples random windows of this size from the data volume. \n",
    "\n",
    "**Exercise**: Implement a data loader that returns windows of size $28 \\times 28$. \n",
    "- Implement the `__init__`, `__getitem__` and `__len__` function of the `EMWindowDataset` class. The initializer should save variables that are necessary for sampling (e.g. the window size). The length of the dataset can simply be a fixed number of iterations (e.g. $10^4$). For sampling, you should pick a random location in the volume and return a window of size $28 \\times 28$. \n",
    "- Make sure that your sampled window is rescaled to the $[-1, 1]$ interval, as with the MNIST data. \n",
    "- The additional code below shows the result of your sampling operation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QFyL6SBv_fZZ"
   },
   "outputs": [],
   "source": [
    "# dataset useful for sampling (and many other things)\n",
    "class EMWindowDataset(data.Dataset):\n",
    "    \n",
    "    def __init__(self, x, wnd_sz, max_iter_epoch):\n",
    "\n",
    "        \"\"\"\n",
    "        INSERT CODE HERE\n",
    "        \"\"\"\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "\n",
    "        \"\"\"\n",
    "        INSERT CODE HERE\n",
    "        \"\"\"\n",
    "\n",
    "        return wnd\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        INSERT CODE HERE\n",
    "        \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "INSERT CODE HERE\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RdIG8CuMdc5w"
   },
   "source": [
    "Now we are reade to train the GAN on EM data! \n",
    "\n",
    "**Exercise**: Below is the `train_gan` function of the MNIST dataset. The only thing you will have to adjust is the data loading step. Keep in mind that the MNIST dataset returns a tuple (the input and a label), whereas our data loader returns only the necessary image. To train the network, we recommend to reduce the learning rate to $10^{-5}$ and increase the amount of epochs to $20$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PQw9UFblYfl0"
   },
   "outputs": [],
   "source": [
    "# train the GAN network for a number of epochs\n",
    "def train_gan(D, G, loader, D_optimizer, G_optimizer, epochs, fixed_z):\n",
    "\n",
    "    loss_d = np.zeros((epochs))\n",
    "    loss_g = np.zeros((epochs))\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # set generator and discriminator to training mode\n",
    "        G.train()\n",
    "        D.train()\n",
    "\n",
    "        for x, _ in loader:\n",
    "            \n",
    "            # get a batch of samples\n",
    "            x = x.view(-1, 28 * 28).to(device).float()\n",
    "\n",
    "            # train discriminator D and cumulate loss\n",
    "            loss = train_discriminator(D, G, x, D_optimizer)\n",
    "            loss_d[epoch] += loss\n",
    "\n",
    "            # train generator G and cumulate loss\n",
    "            loss = train_generator(D, G, G_optimizer)\n",
    "            loss_g[epoch] += loss\n",
    "\n",
    "        # average losses and report\n",
    "        loss_d[epoch] /= len(loader)\n",
    "        loss_g[epoch] /= len(loader)\n",
    "        print('Epoch %d: Discriminator loss = %.3f - Generator loss = %.3f' % (epoch, loss_d[epoch], loss_g[epoch]))\n",
    "\n",
    "        # show generated samples\n",
    "        show_generated_samples(G, fixed_z)\n",
    "\n",
    "    return loss_d, loss_g\n",
    "\n",
    "\n",
    "# parameters\n",
    "z_dim = 64\n",
    "wnd_sz = 28\n",
    "lr = 2e-4\n",
    "epochs = 10\n",
    "\n",
    "# networks\n",
    "G = Generator(input_size=z_dim, output_size=wnd_sz**2).to(device)\n",
    "D = Discriminator(input_size=wnd_sz**2, output_size=1).to(device)\n",
    "\n",
    "# binary cross entropy loss is used in a GAN\n",
    "BCE_loss = nn.BCELoss()\n",
    "\n",
    "# adam optimizer for both networks\n",
    "G_optimizer = optim.Adam(G.parameters(), lr=lr)\n",
    "D_optimizer = optim.Adam(D.parameters(), lr=lr)\n",
    "\n",
    "# fixed noise (for consistency in comparing generated samples)\n",
    "fixed_z = torch.randn((5 * 5, G.input_size)).to(device)\n",
    "\n",
    "# train the network\n",
    "loss_d, loss_g = train_gan(D, G, loader, D_optimizer, G_optimizer, epochs, fixed_z)\n",
    "\n",
    "# visualize the learning curves\n",
    "plt.plot(loss_d / 2)\n",
    "plt.plot(loss_g)\n",
    "plt.legend(('Discriminator', 'Generator'))\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HjzFrWQdZc0z"
   },
   "source": [
    "Lastly, these models allow you to interpolate in a very realistic sense between generated samples. Consider two random noise vectors $\\mathbf{z}$ and $\\mathbf{z}'$, and their corresponding generated samples, $\\mathbf{x}=G(\\mathbf{z})$ and $\\mathbf{x}'=G(\\mathbf{z}')$, respectively. An interpolated sample between the two is then obtained by interpolating in the latent space and decoding with the generator, i.e. $\\mathbf{x}_\\alpha=G(\\alpha \\mathbf{z} + (1 - \\alpha) \\mathbf{z}')$. The code below shows this visually. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4TTEUxEdVk3_"
   },
   "outputs": [],
   "source": [
    "# generate z and z'\n",
    "z = torch.randn((1, G.input_size)).to(device)\n",
    "z_ = torch.randn((1, G.input_size)).to(device)\n",
    "\n",
    "# generate interpolations between z and z'\n",
    "n = 12\n",
    "zi = torch.zeros((n, G.input_size)).to(device)\n",
    "for i in range(1, n+1): \n",
    "    alpha = i / (n+1)\n",
    "    zi[i-1] = alpha * z[0] + (1 - alpha) * z_[0]\n",
    "\n",
    "# concatenate all samples in a single batch\n",
    "z_all = torch.cat((z, zi, z_), dim=0)\n",
    "\n",
    "# decode the samples with the generator\n",
    "x_all = G(z_all)\n",
    "\n",
    "# visualize samples\n",
    "for i in range(n+1): \n",
    "    plt.subplot(1, n+1, i+1)\n",
    "    plt.imshow(x_all[i].detach().cpu().view(28, 28).numpy(), cmap='gray')\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "2020-dlb-6-generative-neural-networks.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
