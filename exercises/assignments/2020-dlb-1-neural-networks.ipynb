{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bSg5U4D5I35S"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JorisRoels/deep-learning-biology/blob/main/exercises/assignments/2020-dlb-1-neural-networks.ipynb)\n",
    "\n",
    "# Exercise 1: Neural Networks\n",
    "\n",
    "In this notebook, we will be using neural networks to identify enzyme sequences from protein sequences. \n",
    "\n",
    "The structure of these exercises is as follows: \n",
    "\n",
    "1. [Import libraries and download data](#scrollTo=ScagUEMTMjlK)\n",
    "2. [Data pre-processing](#scrollTo=ohZHyOTnI35b)\n",
    "3. [Building a neural network with PyTorch](#scrollTo=kIry8iFZI35y)\n",
    "4. [Training & validating the network](#scrollTo=uXrEb0rTI35-)\n",
    "5. [Improving the model](#scrollTo=o76Hxj7-Mst5)\n",
    "6. [Understanding the model](#scrollTo=Ult7CTpCMxTi)\n",
    "\n",
    "This notebook is largely based on the research published in: \n",
    "\n",
    "Li, Y., Wang, S., Umarov, R., Xie, B., Fan, M., Li, L., & Gao, X. (2018). DEEPre: Sequence-based enzyme EC number prediction by deep learning. Bioinformatics, 34(5), 760â€“769. https://doi.org/10.1093/bioinformatics/btx680"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ScagUEMTMjlK"
   },
   "source": [
    "## 1. Import libraries and download data\n",
    "Let's start with importing the necessary libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S1oGi88ZU8eN"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcdefaults()\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from progressbar import ProgressBar, Percentage, Bar, ETA, FileTransferSpeed\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "import gdown\n",
    "import zipfile\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yIPng9wbV-Zs"
   },
   "source": [
    "As you will notice, Colab environments come with quite a large library pre-installed. If you need to import a module that is not yet specified, you can add it in the previous cell (make sure to run it again). If the module is not installed, you can install it with `pip`. \n",
    "\n",
    "To make your work reproducible, it is advised to initialize all modules with stochastic functionality with a fixed seed. Re-running this script should give the same results as long as the seed is fixed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8OOFPFLiV-mh"
   },
   "outputs": [],
   "source": [
    "# make sure the results are reproducible\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# run all computations on the GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Running computations with %s' % torch.device(device))\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_properties(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PjfnM2ffU9G0"
   },
   "source": [
    "We will now download the required data from a public Google Drive repository. The data is stored as a zip archive and automatically extracted to the `data` directory in the current directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Ilt9ZM3I35T"
   },
   "outputs": [],
   "source": [
    "# fields\n",
    "url = 'http://data.bits.vib.be/pub/trainingen/DeepLearning/data-1.zip'\n",
    "cmp_data_path = 'data.zip'\n",
    "\n",
    "# download the compressed data\n",
    "gdown.download(url, cmp_data_path, quiet=False)\n",
    "\n",
    "# extract the data\n",
    "zip = zipfile.ZipFile(cmp_data_path)\n",
    "zip.extractall('')\n",
    "\n",
    "# remove the compressed data\n",
    "os.remove(cmp_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ohZHyOTnI35b"
   },
   "source": [
    "## 2. Data pre-processing\n",
    "\n",
    "The data are protein sequences and stored in binary format as pickle files. However, we encode the data to a binary matrix $X$ where the value at position $(i,j)$ represents the absence or presence of the protein $i$ in the sequence $j$: \n",
    "\n",
    "$$\n",
    "X_{i,j}=\\left\\{\n",
    "        \\begin{array}{ll}\n",
    "          1 \\text{ (protein } i \\text{ is present in sequence } j \\text{)}\\\\\n",
    "          0 \\text{ (protein } i \\text{ is not present in sequence } j \\text{)}\n",
    "        \\end{array} \n",
    "        \\right.\n",
    "$$\n",
    "\n",
    "The corresponding labels $y$ are also binary, they separate the enzyme from the non-enzyme sequences: \n",
    "\n",
    "$$\n",
    "y_{j}=\\left\\{\n",
    "      \\begin{array}{ll}\n",
    "        1 \\text{ (sequence } j \\text{ is an enzyme)}\\\\\n",
    "        0 \\text{ (sequence } j \\text{ is not an enzyme)}\n",
    "      \\end{array} \n",
    "      \\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4hJjZQEGI35f"
   },
   "outputs": [],
   "source": [
    "def encode_data(f_name_list, proteins):\n",
    "    \n",
    "    with open(f_name_list,'rb') as f:\n",
    "        name_list = pickle.load(f)\n",
    "\n",
    "    encoding = []\n",
    "    widgets = ['Encoding data: ', Percentage(), ' ', Bar(), ' ', ETA()]\n",
    "    pbar = ProgressBar(widgets=widgets, maxval=len(name_list))\n",
    "    pbar.start()\n",
    "    for i in range(len(name_list)):\n",
    "        single_encoding = np.zeros(len(proteins))\n",
    "        if name_list[i] != []:\n",
    "            for protein_name in name_list[i]:\n",
    "                single_encoding[proteins.index(protein_name)] = 1\n",
    "        encoding.append(single_encoding)\n",
    "        pbar.update(i)\n",
    "    pbar.finish()\n",
    "    \n",
    "    return np.asarray(encoding, dtype='int8')\n",
    "\n",
    "# specify where the data is stored\n",
    "data_dir = 'data-1/'\n",
    "f_name_list_enzymes = os.path.join(data_dir, 'Pfam_name_list_new_data.pickle')\n",
    "f_name_list_nonenzyme = os.path.join(data_dir, 'Pfam_name_list_non_enzyme.pickle')\n",
    "f_protein_names = os.path.join(data_dir, 'Pfam_model_names_list.pickle')\n",
    "\n",
    "# load the different proteins\n",
    "with open(f_protein_names,'rb') as f:\n",
    "    proteins = pickle.load(f)\n",
    "num_proteins = len(proteins)\n",
    "\n",
    "# encode the sequences to a binary matrix\n",
    "enzymes = encode_data(f_name_list_enzymes, proteins)\n",
    "non_enzymes = encode_data(f_name_list_nonenzyme, proteins)\n",
    "\n",
    "# concatenate everything together\n",
    "X = np.concatenate([enzymes, non_enzymes], axis=0)\n",
    "\n",
    "# the labels are binary (1 for enzymes, 0 for non-enzymes) and are one-hot encoded\n",
    "y = np.concatenate([np.ones([22168,1]), np.zeros([22168,1])], axis=0).flatten()\n",
    "\n",
    "# print a few statistics\n",
    "print('There are %d sequences with %d protein measurements' % (X.shape[0], X.shape[1]))\n",
    "print('There are %d enzyme and %d non-enzyme sequences' % (enzymes.shape[0], non_enzymes.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VChZ6dD9I35l"
   },
   "source": [
    "Here is a quick glimpse in the data. For a random selection of proteins, we plot the amount of times it was counted in the enzyme and non-enzyme sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8alfRhgtI35n"
   },
   "outputs": [],
   "source": [
    "# selection of indices for the proteins\n",
    "inds = np.random.randint(num_proteins, size=20)\n",
    "proteins_subset = [proteins[i] for i in inds]\n",
    "\n",
    "# compute the sum over the sequences\n",
    "enzymes_sum = np.sum(enzymes, axis=1)\n",
    "non_enzymes_sum = np.sum(non_enzymes, axis=1)\n",
    "\n",
    "# plot the counts on the subset of proteins\n",
    "df = pd.DataFrame({'Enzyme': enzymes_sum[inds], 'Non-enzyme': non_enzymes_sum[inds]}, index=proteins_subset)\n",
    "df.plot.barh()\n",
    "plt.xlabel('Counts')\n",
    "plt.ylabel('Protein')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jMI8G95EI35r"
   },
   "source": [
    "To evaluate our approaches properly, we will split the data in a training and testing set. We will use the training set to train our algorithms and the testing set as a separate set of unseen data to evaluate the performance of our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h_t-j1gWI35s"
   },
   "outputs": [],
   "source": [
    "test_ratio = 0.5 # we will use 50% of the data for testing\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=test_ratio, random_state=seed)\n",
    "\n",
    "print('%d sequences for training and %d for testing' % (x_train.shape[0], x_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kIry8iFZI35y"
   },
   "source": [
    "## 3. Building a neural network with PyTorch\n",
    "\n",
    "Now, we have to implement the neural network and train it. For this, we will use the high-level deep learning library [PyTorch](https://pytorch.org/). PyTorch is a well-known, open-source, machine learning framework that has a comprehensive set of tools and libraries and accelerates research prototyping. It also supports transparant training of machine learning models on GPU devices, which can benefit runtimes significantly. The full documentation can be found [here](https://pytorch.org/docs/stable/index.html). \n",
    "\n",
    "Let's start by defining the architecture of neural network. \n",
    "\n",
    "**Exercise**: build a network with a single hidden layer with PyTorch: \n",
    "- The first layer will be a [fully connected layer](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear) with [relu](https://pytorch.org/docs/stable/nn.functional.html?highlight=relu#torch.nn.functional.relu) activation that transforms the input features to a 512 dimensional (hidden) feature vector representation. \n",
    "- The output layer is another [fully connected layer](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear) that transforms the hidden representation to a class probability distribution. \n",
    "- Print the network architecture to validate your architecture. \n",
    "- Run the network on a random batch of samples. Note that you have to transfer the numpy ndarray type inputs to floating point [PyTorch tensors](https://pytorch.org/docs/stable/tensors.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dhB6nbkXI35z"
   },
   "outputs": [],
   "source": [
    "# define the number of classes\n",
    "\"\"\"\n",
    "INSERT CODE HERE\n",
    "\"\"\"\n",
    "\n",
    "# The network will inherit the Module class\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, n_features=512):\n",
    "        super(Net, self).__init__()\n",
    "        \"\"\"\n",
    "        INSERT CODE HERE\n",
    "        \"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        INSERT CODE HERE\n",
    "        \"\"\"\n",
    "        return x\n",
    "\n",
    "# initialize the network and print the architecture\n",
    "\"\"\"\n",
    "INSERT CODE HERE\n",
    "\"\"\"\n",
    "\n",
    "# run the network on a batch of samples\n",
    "# note that we have to transfer the numpy ndarray type inputs to float torch tensors\n",
    "\"\"\"\n",
    "INSERT CODE HERE\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WRgIWPOwI354"
   },
   "source": [
    "**Exercise**: manually compute the amount of parameters in this network and verify this with PyTorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n8l0jAmOI356"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "INSERT CODE HERE\n",
    "\"\"\"\n",
    "print('There are %d trainable parameters in the network' % n_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uXrEb0rTI35-"
   },
   "source": [
    "## 4. Training and validating the network\n",
    "\n",
    "To train this network, we still need two things: a loss function and an optimizer. For the loss function, we will use the commonly used cross entropy loss for classification. For the optimizer, we will use stochastic gradient descent (SGD) with a learning rate of 0.1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8H8AJGfjI35_"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bz8b8lTkI36C"
   },
   "source": [
    "Great. Now it's time to train our model and implement backpropagation. Fortunately, PyTorch makes this relatively easy. A single optimization iteration consists of the following steps: \n",
    "1. Sample a batch from the training data: we use the convenient [data loading](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html) system provided by PyTorch. You can simply enumerate over the `DataLoader` objects. \n",
    "2. Set all gradients equal to zero. You can use the [`zero_grad()`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=zero_grad#torch.nn.Module.zero_grad) function. \n",
    "3. Feed the batch to the network and compute the outputs. \n",
    "4. Compare the outputs to the labels with the loss function. Note that the loss function itself is a `Module` object as well and thus can be treated in a similar fashion as the network for computing outputs. \n",
    "5. Backpropagate the gradients w.r.t. the computed loss. You can use the [`backward()`](https://pytorch.org/docs/stable/autograd.html?highlight=backward#torch.autograd.backward) function for this. \n",
    "6. Apply one step in the optimization (e.g. gradient descent). For this, you will need the optimizer's [`step()`](https://pytorch.org/docs/stable/optim.html#torch.optim.Optimizer.step) function. \n",
    "\n",
    "**Exercise**: train the model with the following settings: \n",
    "- Train the network for 50 epochs\n",
    "- Use a mini batch size of 1024\n",
    "- Track the performance of the classifier by additionally providing the test data. We have already provided a validation function that tracks the accuracy. This function expects a network module, a binary matrix $X$ of sequences, their corresponding labels $y$ and the batch size (for efficiency reasons) as inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jkpn9G8AI36D"
   },
   "outputs": [],
   "source": [
    "# dataset useful for sampling (and many other things)\n",
    "class ProteinSeqDataset(data.Dataset):\n",
    "    \n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.data[i], self.labels[i]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "def validate_accuracy(net, X, y, batch_size=1024):\n",
    "    \n",
    "    # evaluation mode\n",
    "    net.eval()\n",
    "    \n",
    "    # save predictions\n",
    "    y_preds = np.zeros((len(y)))\n",
    "    \n",
    "    for b in range(len(y) // batch_size):\n",
    "        \n",
    "        # sample a batch\n",
    "        inputs = X[b*batch_size: (b+1)*batch_size]\n",
    "    \n",
    "        # transform to tensors\n",
    "        inputs = torch.from_numpy(inputs).float().to(device)\n",
    "\n",
    "        # forward call\n",
    "        y_pred = net(inputs)\n",
    "        y_pred = F.softmax(y_pred, dim=1)[:, 1] > 0.5\n",
    "        \n",
    "        # save predictions\n",
    "        y_preds[b*batch_size: (b+1)*batch_size] = y_pred.detach().cpu().numpy()\n",
    "    \n",
    "    # remaining batch\n",
    "    b = len(y) // batch_size\n",
    "    inputs = torch.from_numpy(X[b*batch_size:]).float().to(device)\n",
    "    y_pred = net(inputs)\n",
    "    y_pred = F.softmax(y_pred, dim=1)[:, 1] > 0.5\n",
    "    y_preds[b*batch_size:] = y_pred.detach().cpu().numpy()\n",
    "    \n",
    "    # compute accuracy\n",
    "    acc = accuracy_score(y, y_preds)\n",
    "    \n",
    "    return acc\n",
    "\n",
    "# implementation of a single training epoch\n",
    "def train_epoch(net, loader, loss_fn, optimizer):\n",
    "\n",
    "    \"\"\"\n",
    "    INSERT CODE HERE\n",
    "    \"\"\"\n",
    "    \n",
    "    return -1\n",
    "\n",
    "# implementation of a single testing epoch\n",
    "def test_epoch(net, loader, loss_fn):\n",
    "\n",
    "    \"\"\"\n",
    "    INSERT CODE HERE\n",
    "    \"\"\"\n",
    "    \n",
    "    return -1\n",
    "\n",
    "def train_net(net, train_loader, test_loader, loss_fn, optimizer, epochs):\n",
    "    \n",
    "    # transfer the network to the GPU\n",
    "    net = net.to(device)\n",
    "    \n",
    "    train_loss = np.zeros((epochs))\n",
    "    test_loss = np.zeros((epochs))\n",
    "    train_acc = np.zeros((epochs))\n",
    "    test_acc = np.zeros((epochs))\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # training\n",
    "        train_loss[epoch] = train_epoch(net, train_loader, loss_fn, optimizer)\n",
    "        train_acc[epoch] = validate_accuracy(net, x_train, y_train)\n",
    "        \n",
    "        # testing\n",
    "        test_loss[epoch] = test_epoch(net, test_loader, loss_fn)\n",
    "        test_acc[epoch] = validate_accuracy(net, x_test, y_test)\n",
    "        \n",
    "        print('Epoch %5d - Train loss: %.6f - Train accuracy: %.6f - Test loss: %.6f - Test accuracy: %.6f' \n",
    "              % (epoch, train_loss[epoch], train_acc[epoch], test_loss[epoch], test_acc[epoch]))\n",
    "    \n",
    "    return train_loss, test_loss, train_acc, test_acc\n",
    "\n",
    "# parameters\n",
    "\"\"\"\n",
    "INSERT CODE HERE\n",
    "\"\"\"\n",
    "\n",
    "# build a training and testing dataloader that handles batch sampling\n",
    "train_data = ProteinSeqDataset(x_train, y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size)\n",
    "test_data = ProteinSeqDataset(x_test, y_test)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "# start training\n",
    "train_loss, test_loss, train_acc, test_acc = train_net(net, train_loader, test_loader, loss_fn, optimizer, n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tK7oPH6TI36P"
   },
   "source": [
    "The code below visualizes the learning curves: these curves illustrate how the loss on the train and test set decays over time. Additionally, we also report a similar curve for the train and test accuracy. The final accuracy is also reported. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W6fR4aCwI36Q"
   },
   "outputs": [],
   "source": [
    "def plot_learning_curves(train_loss, test_loss, train_acc, test_acc):\n",
    "    plt.figure(figsize=(11, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_loss)\n",
    "    plt.plot(test_loss)\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend(('Train', 'Test'))\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_acc)\n",
    "    plt.plot(test_acc)\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.legend(('Train', 'Test'))\n",
    "    plt.show()\n",
    "\n",
    "# plot the learning curves (i.e. train/test loss and accuracy)\n",
    "plot_learning_curves(train_loss, test_loss, train_acc, test_acc)\n",
    "\n",
    "# report final accuracy\n",
    "print('The model obtains an accuracy of %.2f%%' % (100*test_acc[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o76Hxj7-Mst5"
   },
   "source": [
    "## 5. Improving the model\n",
    "\n",
    "We will try to improve the model by improving the training time and mitigating the overfitting to some extent. \n",
    "\n",
    "**Exercise**: Improve the model by implementing the following adjustments: \n",
    "- Train the network based on [`Adam`](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam) optimization instead of stochastic gradient descent. The Adam optimizer adapts its learning rate over time and therefore improves convergence significantly. For more details on the algorithm, we refer to the [original published paper](https://arxiv.org/pdf/1412.6980.pdf). You can significantly reduce the learning rate (e.g. 0.0001) and number of training epochs (e.g. 20). \n",
    "- The first adjustment to avoid overfitting is reduce the size of the network. At first sight this may seem strange because this reduces the capacity of the network. However, large networks are more likely to focus on details in the training data because of the redundant number of neurons in the hidden layer. Experiment with smaller hidden representations (e.g. 32 or 16). \n",
    "- The second adjustment to mitigate overfitting is [Dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html). During training, Dropout layers randomly switch off neurons (i.e. their value is temporarily set to zero). This forces the network to use the other neurons to make an appropriate decision. At test time, the dropout layers are obviously ignored and no neurons are switched off. The amount of neurons that are switched off during training is called the dropout factor (e.g. 0.50). For more details, we refer to the [original published paper](https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FtkzDTniI36V"
   },
   "outputs": [],
   "source": [
    "# The network will inherit the Module class\n",
    "class ImprovedNet(nn.Module):\n",
    "\n",
    "    def __init__(self, n_features=512, p=0.5):\n",
    "        super(ImprovedNet, self).__init__()\n",
    "        \"\"\"\n",
    "        INSERT CODE HERE\n",
    "        \"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        INSERT CODE HERE\n",
    "        \"\"\"\n",
    "        return x\n",
    "\n",
    "# initialize the network and print the architecture\n",
    "\"\"\"\n",
    "INSERT CODE HERE\n",
    "\"\"\"\n",
    "\n",
    "# parameters\n",
    "\"\"\"\n",
    "INSERT CODE HERE\n",
    "\"\"\"\n",
    "\n",
    "# Adam optimization\n",
    "\"\"\"\n",
    "INSERT CODE HERE\n",
    "\"\"\"\n",
    "\n",
    "# start training\n",
    "train_loss, test_loss, train_acc, test_acc = train_net(improved_net, train_loader, test_loader, loss_fn, optimizer, n_epochs)\n",
    "\n",
    "# plot the learning curves (i.e. train/test loss and accuracy)\n",
    "plot_learning_curves(train_loss, test_loss, train_acc, test_acc)\n",
    "\n",
    "# report final accuracy\n",
    "print('The model obtains an accuracy of %.2f%%' % (100*test_acc[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ult7CTpCMxTi"
   },
   "source": [
    "## 6. Understanding the model\n",
    "\n",
    "To gain more insight in the network, it can be useful to take a look to the hidden representations of the network. To do this, you have to propagate a number of samples through the first hidden layer of the network and visualize them using dimensionality reduction techniques. \n",
    "\n",
    "**Exercise**: Visualize the hidden representations of a batch of samples in 2D to gain more insight in the network's decision process: \n",
    "- Compute the hidden representation of a batch of samples. To do this, you will have to select a batch, transform this in a torch Tensor and apply the hidden and relu layer of the network on the inputs. Since these are also modules, you can use them in a similar fashion as the original network. \n",
    "- Extract the outputs of the networks as a numpy array and apply dimensionality reduction. A common dimensionality reducing method is the t-SNE algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tt7F9fwZI36a"
   },
   "outputs": [],
   "source": [
    "# select a batch of samples\n",
    "\"\"\"\n",
    "INSERT CODE HERE\n",
    "\"\"\"\n",
    "\n",
    "# compute the hidden representation of the batch\n",
    "\"\"\"\n",
    "INSERT CODE HERE\n",
    "\"\"\"\n",
    "\n",
    "# reduce the dimensionality of the hidden representations\n",
    "\"\"\"\n",
    "INSERT CODE HERE\n",
    "\"\"\"\n",
    "\n",
    "# visualize the reduced representations and label each sample\n",
    "\"\"\"\n",
    "INSERT CODE HERE\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sN6UbAXdI36f"
   },
   "source": [
    "Another way to analyze the network is by checking which proteins cause the highest hidden activations in enzyme and non-enzyme samples. These features are discriminative for predicting the classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zP1nUqHmxHyJ"
   },
   "outputs": [],
   "source": [
    "# isolate the positive and negative samples\n",
    "h_pos = h[batch_labels == 1]\n",
    "h_neg = h[batch_labels == 0]\n",
    "\n",
    "# compute the mean activation\n",
    "h_pos_mean = h_pos.mean(axis=0)\n",
    "h_neg_mean = h_neg.mean(axis=0)\n",
    "\n",
    "# sort the mean activations\n",
    "i_pos = np.argsort(h_pos_mean)\n",
    "i_neg = np.argsort(h_neg_mean)\n",
    "\n",
    "# select the highest activations\n",
    "n = 5\n",
    "i_pos = i_pos[-n:][::-1]\n",
    "i_neg = i_neg[-n:][::-1]\n",
    "\n",
    "print('Discriminative features that result in high activation for enzyme prediction: ')\n",
    "for i in i_pos:\n",
    "    print('  - %s (mean activation value: %.3f)' % (proteins[i], h_pos_mean[i]))\n",
    "print('Discriminative features that result in high activation for non-enzyme prediction: ')\n",
    "for i in i_neg:\n",
    "    print('  - %s (mean activation value: %.3f)' % (proteins[i], h_neg_mean[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "he0PuLhL4X_x"
   },
   "source": [
    "Understanding how neural networks make decisions is up to this day still an unresolved problem. Especially as the amount of layers increases, the extracted features tend to become more abstract and more challenging to study. If you are interested in this domain, feel free to dig a little deeper in the Explainable AI research domain. For example, [this blogpost](https://medium.com/@shagunm1210/the-explainable-neural-network-8f95256dcddb) might be a good starting point! "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "2020-dlb-1-neural-networks.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
