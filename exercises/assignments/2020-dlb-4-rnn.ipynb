{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bSg5U4D5I35S"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JorisRoels/deep-learning-biology/blob/main/exercises/assignments/2020-dlb-4-rnn.ipynb)\n",
    "\n",
    "# Exercise 4: Recurrent Neural Networks\n",
    "\n",
    "In this notebook, we will be using recurrent neural networks (RNN) to predict secondary structure (SS) from protein sequences. \n",
    "\n",
    "The structure of these exercises is as follows: \n",
    "\n",
    "1. [Import libraries and download data](#scrollTo=ScagUEMTMjlK)\n",
    "2. [Data pre-processing](#scrollTo=9ZKz5lBePiZn)\n",
    "3. [RNN for SS prediction](#scrollTo=TreeLUegygQt)\n",
    "4. [LSTM for SS prediction](#scrollTo=dYrVpP0s0Zm4)\n",
    "5. [Extension with convolutional layers](#scrollTo=PHqTVcZrAPhH)\n",
    "\n",
    "This notebook is largely based on the research published in: \n",
    "\n",
    "Torrisi, M., Kaleel, M., & Pollastri, G. (2019). Deeper Profiles and Cascaded Recurrent and Convolutional Neural Networks for state-of-the-art Protein Secondary Structure Prediction. Scientific Reports. https://doi.org/10.1038/s41598-019-48786-x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ScagUEMTMjlK"
   },
   "source": [
    "## 1. Import libraries and download data\n",
    "Let's start with importing the necessary libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S1oGi88ZU8eN"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import gdown\n",
    "import zipfile\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcdefaults()\n",
    "from sklearn import metrics\n",
    "from progressbar import ProgressBar, Percentage, Bar, ETA, FileTransferSpeed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AAfzQ05HPYP4"
   },
   "source": [
    "As you will notice, Colab environments come with quite a large library pre-installed. If you need to import a module that is not yet specified, you can add it in the previous cell (make sure to run it again). If the module is not installed, you can install it with `pip`. \n",
    "\n",
    "To make your work reproducible, it is advised to initialize all modules with stochastic functionality with a fixed seed. Re-running this script should give the same results as long as the seed is fixed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HAfKTybxrbuy"
   },
   "outputs": [],
   "source": [
    "# make sure the results are reproducible\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# run all computations on the GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Running computations with %s' % torch.device(device))\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_properties(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qss19UvVNkOV"
   },
   "source": [
    "We will now download the required data from a public Google Drive repository. The data is stored as a zip archive and automatically extracted to the `data` directory in the current directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yfGy8mGSryCf"
   },
   "outputs": [],
   "source": [
    "# fields\n",
    "url = 'http://data.bits.vib.be/pub/trainingen/DeepLearning/data-4.zip'\n",
    "cmp_data_path = 'data.zip'\n",
    "\n",
    "# download the compressed data\n",
    "gdown.download(url, cmp_data_path, quiet=False)\n",
    "\n",
    "# extract the data\n",
    "zip = zipfile.ZipFile(cmp_data_path)\n",
    "zip.extractall('')\n",
    "\n",
    "# remove the compressed data\n",
    "os.remove(cmp_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ZKz5lBePiZn"
   },
   "source": [
    "## 2. Data visualization and pre-processing\n",
    "\n",
    "The data used for this exercise session originates from the Protein Data Bank (PDB), a public repository of all freely and publicly known protein structures. Eight classes of secondary structure were annotated: helices (G, H, I), sheets (E, B) and coils (C, S, T). \n",
    "\n",
    "Let's start by extracting and loading the data. This notebook can be relatively RAM intensive. By default, you will only use 30% of the available data. If you notice crashes due to RAM overuse, please reduce the `frac` parameter below. You will use less data, and therefore less RAM, so the results will be slightly worse, but your conclusions should not be affected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ou7IZcZoLDbf"
   },
   "outputs": [],
   "source": [
    "# function that loads and preprocesses the dataset\n",
    "def load_dataset(f, frac=0.5):\n",
    "\n",
    "    file = open(f, 'r') \n",
    "\n",
    "    # read in header\n",
    "    n_proteins = int(file.readline())\n",
    "    input_per_AA_classess = file.readline()\n",
    "    data = file.readlines()\n",
    "\n",
    "    # select a fraction of the data (due to RAM limitations, should be a multiple of 5)\n",
    "    max_lines = (int(len(data) * frac) // 5) * 5\n",
    "    data = data[:max_lines]\n",
    "\n",
    "    # read in the actual data\n",
    "    pdb_ids = []\n",
    "    lengths = []\n",
    "    encoded_inputs = []\n",
    "    secondary_structures = []\n",
    "    widgets = ['Loading data: ', Percentage(), ' ', Bar(), ' ', ETA()]\n",
    "    pbar = ProgressBar(widgets=widgets, maxval=len(data))\n",
    "    pbar.start()\n",
    "    for i, line in enumerate(data):\n",
    "        line = line.replace('\\n', '')\n",
    "        if i % 5 == 0:\n",
    "            pdb_ids.append(line)\n",
    "        elif i % 5 == 1:\n",
    "            length = int(line)\n",
    "            lengths.append(length)\n",
    "        elif i % 5 == 2:\n",
    "            encoded_input = np.asarray(line.split(' '), dtype=float).reshape((length, 22))\n",
    "            encoded_inputs.append(encoded_input) \n",
    "        elif i % 5 == 3:\n",
    "            secondary_structure = np.asarray(line[:-1].split(' '), dtype=int)\n",
    "            secondary_structures.append(secondary_structure)\n",
    "        pbar.update(i)\n",
    "    pbar.finish()\n",
    "\n",
    "    # save the lengths as a numpy array\n",
    "    lengths = np.asarray(lengths, dtype=int)\n",
    "\n",
    "    # pad all sequences to the length of the longest sequence (with zeros)\n",
    "    # the corresponding secondary labels are padded with negative values\n",
    "    max_length = lengths.max()\n",
    "    widgets = ['Padding data: ', Percentage(), ' ', Bar(), ' ', ETA()]\n",
    "    pbar = ProgressBar(widgets=widgets, maxval=len(encoded_inputs))\n",
    "    pbar.start()\n",
    "    for i, ei in enumerate(encoded_inputs):\n",
    "        encoded_inputs[i] = np.pad(ei, ((0, max_length - ei.shape[0]), (0, 0)), 'constant', constant_values=0)\n",
    "        secondary_structures[i] = np.pad(secondary_structures[i], ((0, max_length - secondary_structures[i].shape[0])), \n",
    "                                        'constant', constant_values=-1)\n",
    "        pbar.update(i)\n",
    "    pbar.finish()\n",
    "\n",
    "    # save the data in numpy arrays\n",
    "    encoded_inputs = np.asarray(encoded_inputs)\n",
    "    secondary_structures = np.asarray(secondary_structures, dtype=int)\n",
    "\n",
    "    return pdb_ids, encoded_inputs, secondary_structures, lengths\n",
    "\n",
    "# path to the data\n",
    "f_train = 'data-4/train-vHHblits.dataset'\n",
    "f_test = 'data-4/test-vHHblits.dataset'\n",
    "\n",
    "# load only a fraction due to RAM limitations\n",
    "frac = 0.3\n",
    "\n",
    "# load the data\n",
    "pdb_ids_train, encoded_inputs_train, secondary_structures_train, lengths_train = load_dataset(f_train, frac=frac)\n",
    "pdb_ids_test, encoded_inputs_test, secondary_structures_test, lengths_test = load_dataset(f_test, frac=frac)\n",
    "\n",
    "# mappings of the class indices to secondary structure labels\n",
    "mappings = ['G', 'H', 'I', 'E', 'B', 'C', 'S', 'T']\n",
    "\n",
    "# print out basic statistics\n",
    "print()\n",
    "print('There are %d protein sequences in the training set, with an average length of %.2f. The class distribution is as follows: ' % (encoded_inputs_train.shape[0], lengths_train.mean()))\n",
    "for i, label in enumerate(mappings):\n",
    "    print('%s: %.2f%%' % (label, (secondary_structures_train == i).sum() / (secondary_structures_train >= 0).sum() * 100))\n",
    "print('There are %d protein sequences in the testing set, with an average length of %.2f. The class distribution is as follows: ' % (encoded_inputs_test.shape[0], lengths_test.mean()))\n",
    "for i, label in enumerate(mappings):\n",
    "    print('%s: %.2f%%' % (label, (secondary_structures_test == i).sum() / (secondary_structures_test >= 0).sum() * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vwZ7Hk3TcW_D"
   },
   "source": [
    "Even though the data is quite imbalanced, it turns out we can predict one of these eight classes relatively accurate (see the paper reference at the start of this notebook). In this tutorial, however, we will simplify the problem a little bit by merging the secondary structures up to the level of helices, sheets and coils. The following block of code performs this merging step and prints out the class distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pqljlOPRYwAv"
   },
   "outputs": [],
   "source": [
    "# group labels to helices, sheets and coils for simplicity\n",
    "secondary_structures_group_train = np.copy(secondary_structures_train)\n",
    "helices = (secondary_structures_train == 0) + (secondary_structures_train == 1) + (secondary_structures_train == 2)\n",
    "secondary_structures_group_train[helices] = 0\n",
    "sheets = (secondary_structures_train == 3) + (secondary_structures_train == 4)\n",
    "secondary_structures_group_train[sheets] = 1\n",
    "coils = (secondary_structures_train == 5) + (secondary_structures_train == 6) + (secondary_structures_train == 7)\n",
    "secondary_structures_group_train[coils] = 2\n",
    "\n",
    "# same for test set\n",
    "secondary_structures_group_test = np.copy(secondary_structures_test)\n",
    "helices = (secondary_structures_test == 0) + (secondary_structures_test == 1) + (secondary_structures_test == 2)\n",
    "secondary_structures_group_test[helices] = 0\n",
    "sheets = (secondary_structures_test == 3) + (secondary_structures_test == 4)\n",
    "secondary_structures_group_test[sheets] = 1\n",
    "coils = (secondary_structures_test == 5) + (secondary_structures_test == 6) + (secondary_structures_test == 7)\n",
    "secondary_structures_group_test[coils] = 2\n",
    "\n",
    "# print out statistics of the grouped labels\n",
    "mappings_hl = ['helices', 'sheets', 'coils']\n",
    "print()\n",
    "print('The grouped class distribution in the training set is as follows: ')\n",
    "for i, label in enumerate(mappings_hl):\n",
    "    print('%s: %.2f%%' % (label, (secondary_structures_group_train == i).sum() / (secondary_structures_group_train >= 0).sum() * 100))\n",
    "print('The grouped class distribution in the training set is as follows: ')\n",
    "for i, label in enumerate(mappings_hl):\n",
    "    print('%s: %.2f%%' % (label, (secondary_structures_group_test == i).sum() / (secondary_structures_group_test >= 0).sum() * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dKfTV_MQdV8H"
   },
   "source": [
    "It seems that the sheets class is slightly under-represented. We will correct this by using the following class weights: \n",
    "$$w_c = \\frac{N}{C \\times N_c}$$\n",
    "where $N=4725$ is the total amount of training samples, $C=3$ the amount of classes and $N_c$ is the amount of samples that belong to class $c$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pBsKnjAHNpIH"
   },
   "outputs": [],
   "source": [
    "# computing balancing weights\n",
    "t = (secondary_structures_group_train >= 0).sum()\n",
    "w = [t / (3 * (secondary_structures_group_train == i).sum()) for i in range(3)]\n",
    "w = np.asarray(w)\n",
    "print()\n",
    "print('Class balancing weights: ')\n",
    "for i, label in enumerate(mappings_hl):\n",
    "    print('%s: %.2f' % (label, w[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xWrq0A0fE3TL"
   },
   "source": [
    "As you can see, the sheets class will be assigned a larger weight to cope with its slight under-representation. \n",
    "\n",
    "Now that we have our data in order, it is time to define a dataloader that can be used for our predictive recurrent networks. The dataloader should return sequences of encodings and their corresponding labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PLhWCvnA0C1n"
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "\n",
    "train = TensorDataset(torch.from_numpy(encoded_inputs_train).float(), \n",
    "                      torch.from_numpy(secondary_structures_group_train))\n",
    "test = TensorDataset(torch.from_numpy(encoded_inputs_test).float(), \n",
    "                     torch.from_numpy(secondary_structures_group_test))\n",
    "\n",
    "dataloader_train = DataLoader(train, batch_size=batch_size)\n",
    "dataloader_test = DataLoader(test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qLicP2pJX9RC"
   },
   "outputs": [],
   "source": [
    "i = 2\n",
    "x, y = train[i]\n",
    "x = x[:lengths_train[i]]\n",
    "y = y[:lengths_train[i]]\n",
    "print('Sequence %d:' % i)\n",
    "print('    - input shape = %s' % str(list(x.shape)))\n",
    "print('    - output shape = %s' % str(list(y.shape)))\n",
    "print('    - length = %d' % x.shape[0])\n",
    "print('    - AA-encoding length = %d' % x.shape[1])\n",
    "print('    - padded length = %d' % train[i][0].shape[0])\n",
    "for i in range(3):\n",
    "    plt.scatter(np.arange(len(y))[y==i], y[y==i])\n",
    "plt.legend(mappings_hl)\n",
    "plt.yticks(np.arange(3))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OfIWcgw2G0Re"
   },
   "source": [
    "This plot shows the dimensions of a particular input sequence. Each amino-acid (AA) is encoded as a vector of length $A=22$. Each AA has a corresponding secondary structure (helix, sheet or coil). The protein sequences are variable length, however, we have padded the sequences to match with the longest sequence. The corresponding secondary structure labels for these paddings were set to -1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TreeLUegygQt"
   },
   "source": [
    "## 3. RNN for SS prediction\n",
    "\n",
    "Now we are ready to define a recurrent network that can process the protein sequences and predict the corresponding SS label. The model will consist of an RNN that takes an input sequence and maps this to a hidden state. This hidden state can then be fed to a linear layer to predict the SS of the protein sequence. You can chose the dimensionality of the hidden state yourself. \n",
    "\n",
    "**Exercise**: implement the `RNNNet` class, more specifically: \n",
    "- Implement the initializer by defining the correct layers: you will need an [RNN](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html) layer and a [Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) layer that performs classification. \n",
    "- Implement the forward propagation function. \n",
    "- Evaluate the performance by plotting the learning curves and showing the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MDqB685EUpUS"
   },
   "outputs": [],
   "source": [
    "class RNNNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(RNNNet, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        INSERT CODE HERE\n",
    "        \"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        \"\"\"\n",
    "        INSERT CODE HERE\n",
    "        \"\"\"\n",
    "        \n",
    "        return x\n",
    "\n",
    "rnn = RNNNet().to(device)\n",
    "print(rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HlinkdJX19qs"
   },
   "source": [
    "You can use the code below to train and evaluate your implemented model. For a given input sequence, your model should return a sequence of class probability distributions. More precisely, for a batch size $B$ and sequence of length $S$, the input has shape $[B, S, A]$ and the output has shape $[B, S, C]$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nNbX6ERyVlIA"
   },
   "outputs": [],
   "source": [
    "# evaluates the performance of a model and return the predictions and ground truth\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    outputs = []\n",
    "    predictions = []\n",
    "    for i, batch in enumerate(loader):\n",
    "        sequences = batch[0].to(device)\n",
    "        output = batch[1].to(device)\n",
    "        prediction = model(sequences)\n",
    "        \n",
    "        output = output.flatten()\n",
    "        mask = output >= 0\n",
    "        output = output[mask]\n",
    "        prediction = prediction.argmax(dim=2).flatten()\n",
    "        prediction = prediction[mask]\n",
    "        \n",
    "        predictions.append(prediction.detach().cpu().numpy())\n",
    "        outputs.append(output.detach().cpu().numpy())\n",
    "    \n",
    "    return np.hstack(predictions), np.hstack(outputs)\n",
    "\n",
    "# implementation of a single training epoch\n",
    "def train_epoch(net, loader, loss_fn, optimizer):\n",
    "    \n",
    "    # set the network in training mode\n",
    "    net.train()\n",
    "    \n",
    "    # keep track of the loss\n",
    "    loss_cum = 0\n",
    "    cnt = 0\n",
    "    \n",
    "    for i, data in enumerate(loader): \n",
    "        \n",
    "        # sample data\n",
    "        x, y = data\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        # set all gradients equal to zero\n",
    "        net.zero_grad()\n",
    "        \n",
    "        # feed the batch to the network and compute the outputs\n",
    "        y_pred = net(x)\n",
    "        \n",
    "        # compare the outputs to the labels with the loss function\n",
    "        loss = loss_fn(y_pred.view(-1, y_pred.size(2)), y.flatten())\n",
    "        loss_cum += loss.data.cpu().numpy()\n",
    "        cnt += 1\n",
    "        \n",
    "        # backpropagate the gradients w.r.t. computed loss\n",
    "        loss.backward()\n",
    "\n",
    "        # apply one step in the optimization\n",
    "        optimizer.step()\n",
    "    \n",
    "    # compute the average loss\n",
    "    loss_avg = loss_cum / cnt\n",
    "    \n",
    "    return loss_avg\n",
    "\n",
    "# implementation of a single testing epoch\n",
    "def test_epoch(net, loader, loss_fn):\n",
    "    \n",
    "    # set the network in evaluation mode\n",
    "    net.eval()\n",
    "    \n",
    "    # keep track of the loss\n",
    "    loss_cum = 0\n",
    "    cnt = 0\n",
    "    \n",
    "    for i, data in enumerate(loader): \n",
    "        \n",
    "        # sample data\n",
    "        x, y = data\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        # feed the batch to the network and compute the outputs\n",
    "        y_pred = net(x)\n",
    "        \n",
    "        # compare the outputs to the labels with the loss function\n",
    "        loss = loss_fn(y_pred.view(-1, y_pred.size(2)), y.flatten())\n",
    "        loss_cum += loss.data.cpu().numpy()\n",
    "        cnt += 1\n",
    "    \n",
    "    # compute the average loss\n",
    "    loss_avg = loss_cum / cnt\n",
    "    \n",
    "    return loss_avg\n",
    "\n",
    "def train_net(net, train_loader, test_loader, loss_fn, optimizer, epochs):\n",
    "    \n",
    "    # transfer the network to the GPU\n",
    "    net = net.to(device)\n",
    "    \n",
    "    train_loss = np.zeros((epochs))\n",
    "    test_loss = np.zeros((epochs))\n",
    "    train_acc = np.zeros((epochs))\n",
    "    test_acc = np.zeros((epochs))\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # training\n",
    "        train_loss[epoch] = train_epoch(net, train_loader, loss_fn, optimizer)\n",
    "        predictions, outputs = evaluate(net, train_loader)\n",
    "        train_acc[epoch] = metrics.balanced_accuracy_score(outputs, predictions)\n",
    "        \n",
    "        # testing\n",
    "        test_loss[epoch] = test_epoch(net, test_loader, loss_fn)\n",
    "        predictions, outputs = evaluate(net, test_loader)\n",
    "        test_acc[epoch] = metrics.balanced_accuracy_score(outputs, predictions)\n",
    "        \n",
    "        print('Epoch %5d - Train loss: %.6f - Train accuracy: %.6f - Test loss: %.6f - Test accuracy: %.6f' \n",
    "              % (epoch, train_loss[epoch], train_acc[epoch], test_loss[epoch], test_acc[epoch]))\n",
    "    \n",
    "    return (train_loss, test_loss), (train_acc, test_acc)\n",
    "\n",
    "epochs = 10\n",
    "lr = 1e-2\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=-1, weight=torch.from_numpy(w).to(device).float())\n",
    "optimizer = optim.Adam(rnn.parameters(), lr=lr)\n",
    "\n",
    "loss_rnn, acc_rnn = train_net(rnn, dataloader_train, dataloader_test, loss_fn, optimizer, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pQN52ZIHa52R"
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (15,5)\n",
    "\n",
    "# plot train/test loss curves\n",
    "\"\"\"\n",
    "INSERT CODE HERE\n",
    "\"\"\"\n",
    "\n",
    "# plot train/test balanced accuracy\n",
    "\"\"\"\n",
    "INSERT CODE HERE\n",
    "\"\"\"\n",
    "\n",
    "# show the confusion matrix\n",
    "\"\"\"\n",
    "INSERT CODE HERE\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVmFrgHFy9ou"
   },
   "source": [
    "That seems not too bad. You should be able to achieve a test balanced accuracy of about 60%. The training curve also indicates that we might be able to get higher test accuracy by training it a little longer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dYrVpP0s0Zm4"
   },
   "source": [
    "## 3. LSTM for SS prediction\n",
    "\n",
    "Now that we have a working RNN, we can try and improve it by changing it into an LSTM. \n",
    "\n",
    "**Exercise**: implement the `LSTMNet` class, more specifically: \n",
    "- Implement the initializer by defining the correct layers: you will need an [LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html) layer and a [Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) layer that performs classification. \n",
    "- Implement the forward propagation function. \n",
    "- Make the LSTM bidirectional and evaluate the performance\n",
    "- Change the hidden size and evaluate the performance\n",
    "- Change the number of layers in the LSTM, i.e. the amount of LSTMs stacked on top of each other and evaluate the performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ywMjZKxAIkoL"
   },
   "outputs": [],
   "source": [
    "class LSTMNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size=16, bidirectional=False, num_layers=1):\n",
    "        super(LSTMNet, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        INSERT CODE HERE\n",
    "        \"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        \"\"\"\n",
    "        INSERT CODE HERE\n",
    "        \"\"\"\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "INSERT CODE HERE\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EEpGb8j7dCBe"
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (15,5)\n",
    "\n",
    "# plot test loss curves\n",
    "\"\"\"\n",
    "INSERT CODE HERE\n",
    "\"\"\"\n",
    "\n",
    "# plot test balanced accuracy\n",
    "\"\"\"\n",
    "INSERT CODE HERE\n",
    "\"\"\"\n",
    "\n",
    "# show the confusion matrix of the best performing model\n",
    "\"\"\"\n",
    "INSERT CODE HERE\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IY1mXuJXsCVM"
   },
   "source": [
    "It could take a while to train all these different networks, so preferably chose a sufficiently high learning rate. From the experiments, it turns out that the LSTM network is outperforming the standard RNN network. Given the theory, this is also according to our expectations. Moreover, we see that wider hidden units and stacked LSTMs can benefit the results. However, the bidirectional LSTMs turn out to improve the results the most in this case. The best performing method that combines all these improvements (wider, stacked and bidirectional) scores a balanced accuracy of over 70%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PHqTVcZrAPhH"
   },
   "source": [
    "## 4. Extension with convolutional layers\n",
    "\n",
    "Convolutional layers can be used to improve the model that we currently have. The idea is that a convolutional layer can provide a good embedding and that the recurrent network exploits this. The architecture that we will implement works as follows: the input is fed to a ReLU activated 1D convolutional layer (along the sequence dimension), a recurrent network (we will use an LSTM) and two linear layers for classification (the first one is ReLU activated). The figure below shows the architecture. \n",
    "\n",
    "![Image](https://i.ibb.co/MgVtHgH/scheme.png)\n",
    "\n",
    "**Exercise**: Implement the architecture described in the top figure. You will need a [Conv1d](https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html), [ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html), [LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html) layer and two [Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) layers. Make sure the convolution is applied along the sequence dimension. The [`permute`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.permute) function can be useful for this. Evaluate the performance and compare to the previous models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vcROQVTvbJ3c"
   },
   "outputs": [],
   "source": [
    "class ImprovedLSTMNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size=16, bidirectional=False, num_layers=1, conv_channels=16, linear_channels=16):\n",
    "        super(ImprovedLSTMNet, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        INSERT CODE HERE\n",
    "        \"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        \"\"\"\n",
    "        INSERT CODE HERE\n",
    "        \"\"\"\n",
    "        \n",
    "        return x\n",
    "\n",
    "lstm = ImprovedLSTMNet(bidirectional=True, hidden_size=32, num_layers=2).to(device)\n",
    "\n",
    "print(lstm)\n",
    "optimizer = optim.Adam(lstm.parameters(), lr=lr)\n",
    "\n",
    "loss_lstm, acc_lstm = train_net(lstm, dataloader_train, dataloader_test, loss_fn, optimizer, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0aUQw9qLLpI6"
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (15,5)\n",
    "\n",
    "# plot test loss curves\n",
    "\"\"\"\n",
    "INSERT CODE HERE\n",
    "\"\"\"\n",
    "\n",
    "# plot test balanced accuracy\n",
    "\"\"\"\n",
    "INSERT CODE HERE\n",
    "\"\"\"\n",
    "\n",
    "# show the confusion matrix of the best performing model\n",
    "\"\"\"\n",
    "INSERT CODE HERE\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FLcFsj6L5h-g"
   },
   "source": [
    "Looks like we have improved the results to 75%! Convolutional layers seem to be great embedding layers for sequence data. It turns out that you can actually achieve 84% accuracy on this dataset (Torrisi et.al., 2019), by using all available training data (we only used a fraction due to RAM limits), increasing the depth/width of the model, more epochs, etc. Feel free to try this out on your own if you have the resources available! \n",
    "\n",
    "Note that your results might differ a little due to the different initialization that you might have used. By fixing the seed in the first code cell and running everything from scratch, the results should be reproducible. "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "2020-dlb-4-rnn.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
